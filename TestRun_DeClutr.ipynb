{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8779a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
      "\u001b[K     |██████████████████████████████  | 1915.4 MB 58.4 MB/s eta 0:00:03     |████████████████▌               | 1054.4 MB 59.5 MB/s eta 0:00:17     |██████████████████▉             | 1201.5 MB 65.2 MB/s eta 0:00:13     |█████████████████████▌          | 1372.4 MB 78.6 MB/s eta 0:00:09"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 2041.3 MB 114.4 MB/s eta 0:00:01\u001b[K     |████████████████████████████████| 2041.3 MB 15 kB/s \n",
      "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio==0.9.0\n",
      "  Using cached torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/envs/con_env1/lib/python3.7/site-packages (from torch==1.9.0+cu111) (4.1.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in ./anaconda3/envs/con_env1/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (9.1.1)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/con_env1/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (1.21.6)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.1+cu110\n",
      "    Uninstalling torch-1.7.1+cu110:\n",
      "      Successfully uninstalled torch-1.7.1+cu110\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.12.0\n",
      "    Uninstalling torchvision-0.12.0:\n",
      "      Successfully uninstalled torchvision-0.12.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.11.0\n",
      "    Uninstalling torchaudio-0.11.0:\n",
      "      Successfully uninstalled torchaudio-0.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "allennlp 1.1.0 requires torch<1.7.0,>=1.6.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc41a59",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (118173973.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_85373/118173973.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wandb login\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201a257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guneetsk99/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guneetsk99/wandb/run-20220618_182903-36sllx7l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/guneetsk99/my_first_test/runs/36sllx7l\" target=\"_blank\">gallant-thunder-1</a></strong> to <a href=\"https://wandb.ai/guneetsk99/my_first_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/guneetsk99/my_first_test/runs/36sllx7l?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f11144c0fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='my_first_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb1edebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at johngiorgi/declutr-base were not used when initializing RobertaModel: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using custom data configuration tomekkorbak--pile-nontoxic-chunk-0-52be9de9481b387c\n",
      "Reusing dataset parquet (/home/guneetsk99/.cache/huggingface/datasets/tomekkorbak___parquet/tomekkorbak--pile-nontoxic-chunk-0-52be9de9481b387c/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77a6650e8764ed5ba62d06f9a350a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"johngiorgi/declutr-base\")\n",
    "model = AutoModel.from_pretrained(\"johngiorgi/declutr-base\" ).to('cuda')\n",
    "\n",
    "# Prepare some text to embed\n",
    "dataset = load_dataset(\"tomekkorbak/pile-nontoxic-chunk-0\")\n",
    "train_dataset = Dataset.from_dict(dataset['train'].to_dict())\n",
    "# small dataset with 1000 samples\n",
    "small_dataset = train_dataset.select([i for i in range(9000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d254f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_pooling(model_output, sample):\n",
    "    return torch.sum(\n",
    "        model_output * sample[\"attention_mask\"].unsqueeze(-1), dim=1\n",
    "        ) / torch.clamp(torch.sum(sample[\"attention_mask\"], dim=1, keepdims=True), min=1e-9)\n",
    "        \n",
    "def get_embeddings(examples):\n",
    "    text_list = examples['text']\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512\n",
    "    ).to('cuda')\n",
    "    model_output = model(**encoded_input)[0]\n",
    "    return mean_pooling(model_output, encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63397a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d119e747a91c42aba40f2650435f3405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = small_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x).cpu().detach().numpy()[0]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbe5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "np_arr=numpy.array(embeddings_dataset['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cb7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 18:35:20,759 [INFO]: Using 96 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2022-06-18 18:35:20,798 [INFO]: Launching the whole pipeline 06/18/2022, 18:35:20\n",
      "2022-06-18 18:35:20,798 [INFO]: Reading total number of vectors and dimension 06/18/2022, 18:35:20\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "2022-06-18 18:35:20,939 [INFO]: There are 9000 embeddings of dim 768\n",
      "2022-06-18 18:35:20,939 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.1396 secs\n",
      "2022-06-18 18:35:20,940 [INFO]: \tCompute estimated construction time of the index 06/18/2022, 18:35:20\n",
      "2022-06-18 18:35:20,940 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2022-06-18 18:35:20,941 [INFO]: \t\t-> Add: 0.1 seconds\n",
      "2022-06-18 18:35:20,941 [INFO]: \t\tTotal: 16.7 minutes\n",
      "2022-06-18 18:35:20,942 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0013 secs\n",
      "2022-06-18 18:35:20,942 [INFO]: \tChecking that your have enough memory available to create the index 06/18/2022, 18:35:20\n",
      "2022-06-18 18:35:20,943 [INFO]: 30.1MB of memory will be needed to build the index (more might be used if you have more)\n",
      "2022-06-18 18:35:20,943 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0011 secs\n",
      "2022-06-18 18:35:20,944 [INFO]: \tSelecting most promising index types given data characteristics 06/18/2022, 18:35:20\n",
      "2022-06-18 18:35:20,944 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "2022-06-18 18:35:20,945 [INFO]: \tCreating the index 06/18/2022, 18:35:20\n",
      "2022-06-18 18:35:20,945 [INFO]: \t\t-> Instanciate the index HNSW15 06/18/2022, 18:35:20\n",
      "2022-06-18 18:35:20,946 [INFO]: \t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0004 secs\n",
      "2022-06-18 18:35:20,946 [INFO]: The index size will be approximately 27.4MB\n",
      "2022-06-18 18:35:20,946 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n",
      "2022-06-18 18:35:20,947 [INFO]: Will be using at most 1GB of ram for adding\n",
      "2022-06-18 18:35:20,947 [INFO]: \t\t-> Adding the vectors to the index 06/18/2022, 18:35:20\n",
      "2022-06-18 18:35:20,947 [INFO]: Using a batch size of 325520 (memory overhead 953.7MB)\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  6.62it/s]\n",
      "2022-06-18 18:35:21,254 [INFO]: \tComputing best hyperparameters for index /home/guneetsk99/knn.index 06/18/2022, 18:35:21\n",
      "2022-06-18 18:35:32,746 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /home/guneetsk99/knn.index\" in 11.4912 secs\n",
      "2022-06-18 18:35:32,751 [INFO]: The best hyperparameters are: efSearch=16\n",
      "2022-06-18 18:35:32,752 [INFO]: \tCompute fast metrics 06/18/2022, 18:35:32\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "2022-06-18 18:35:42,938 [INFO]: \t>>> Finished \"Compute fast metrics\" in 10.1857 secs\n",
      "2022-06-18 18:35:42,940 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 21.9925 secs\n",
      "2022-06-18 18:35:42,940 [INFO]: {\n",
      "2022-06-18 18:35:42,941 [INFO]: \tindex_key: HNSW15\n",
      "2022-06-18 18:35:42,941 [INFO]: \tindex_param: efSearch=16\n",
      "2022-06-18 18:35:42,942 [INFO]: \tindex_path: /home/guneetsk99/knn.index\n",
      "2022-06-18 18:35:42,942 [INFO]: \tsize in bytes: 28872730\n",
      "2022-06-18 18:35:42,942 [INFO]: \tavg_search_speed_ms: 13.36026061836068\n",
      "2022-06-18 18:35:42,943 [INFO]: \t99p_search_speed_ms: 19.323481898754835\n",
      "2022-06-18 18:35:42,943 [INFO]: \treconstruction error %: 0.0\n",
      "2022-06-18 18:35:42,943 [INFO]: \tnb vectors: 9000\n",
      "2022-06-18 18:35:42,944 [INFO]: \tvectors dimension: 768\n",
      "2022-06-18 18:35:42,944 [INFO]: \tcompression ratio: 0.95758177352817\n",
      "2022-06-18 18:35:42,944 [INFO]: }\n",
      "2022-06-18 18:35:42,945 [INFO]: \t>>> Finished \"Creating the index\" in 21.9997 secs\n",
      "2022-06-18 18:35:42,945 [INFO]: >>> Finished \"Launching the whole pipeline\" in 22.1466 secs\n"
     ]
    }
   ],
   "source": [
    "from autofaiss import build_index\n",
    "index, index_infos = build_index(np_arr, save_on_disk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b414f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
